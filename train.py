import pandas as pd
import numpy as np
import os
import cv2
import tensorflow as tf
import glob
import xml.etree.ElementTree as ET
from collections import namedtuple, OrderedDict
import io
from PIL import Image
from object_detection.utils import dataset_util
import wget
import tarfile
from object_detection.utils import config_util
from object_detection.protos import pipeline_pb2
from google.protobuf import text_format
from utils import bar_progress

#!!!!!!!!!!!!!!!!!Скачать отсюда папку research в основную директорию!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#                         https://github.com/tensorflow/models


train_data_folder = 'head_train'
train_data_folder = os.path.join('data', train_data_folder)
#Нужно ли создавать датасет
create_dataset = False
#Папка с изображениями и xml
path_to_imgs_xml = 'D:/ML/CV_Project/Head_detector/imgs_xml'
# Размер изображений
height = 1080
width = 1920
img_shape = [height, width]

num_classes = 1

#Ссылка на pre-trained модель. None если модель уже есть
# model_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz'
model_url = None
#Имя модели. Новое или той, которая уже есть. None - стандартное название
# model_name = 'ssd_resnet101_v1_fpn_1024x1024'
model_name = 'ssd_mobilenet_v2_fpnlite_640x640'


#Изменить только номер, изначально 0. Новые checkpoint сохраняются в папку с самой моделью
#Оба файла с нужным названием скопировать в папку checkpoints
ckpt_file = 'ckpt-15'
# Параметры для тренировки
batch_size = 5
lr = 0.0001
lr_decay = 0.00001
# Суммарное количество шагов. Начальный шаг берется из checkpoint файла
num_steps = 20000

if not os.path.exists('data'):
    os.mkdir('data')

if not os.path.exists(train_data_folder):
    os.mkdir(train_data_folder)

if (not model_name) and model_url:
    model_name = model_url.split('/')[-1]
    model_name = model_name.split('.')[0]
assert not os.path.exists(f'pretrained_models{model_name}'), 'Такая модель уже существует'
assert model_name or model_url, 'Введите url или имя существующей модели'
#Файл с TF Records
tf_records_path = os.path.join(train_data_folder,'heads.records')

#--------------------------------------------
#Подготовка датасета

def xml_to_csv(path, img_shape=[None, None]):
    """Iterates through all .xml files (generated by labelImg) in a given directory and combines
    them in a single Pandas dataframe.
    """
    height, width = img_shape
    xml_list = []
    for xml_file in glob.glob(path + '/*.xml'):
        # print(xml_file)
        tree = ET.parse(xml_file)
        root = tree.getroot()
        filename = root.find('filename').text
        filename = f'{path_to_imgs_xml}/{filename}'
        if not (height and width):
            width = int(root.find('size').find('width').text)
            height = int(root.find('size').find('height').text)
        for member in root.findall('object'):
            bndbox = member.find('bndbox')
            value = (filename,
                     width,
                     height,
                     member.find('name').text,
                     int(bndbox.find('xmin').text),
                     int(bndbox.find('ymin').text),
                     int(bndbox.find('xmax').text),
                     int(bndbox.find('ymax').text),
                     )
            xml_list.append(value)
    column_name = ['filename', 'width', 'height',
                   'class', 'xmin', 'ymin', 'xmax', 'ymax']
    xml_df = pd.DataFrame(xml_list, columns=column_name)
    return xml_df

def split(df, group):
    data = namedtuple('data', ['filename', 'object'])
    gb = df.groupby(group)
    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]


def create_tf_example(group, labels_to_int):
    with tf.io.gfile.GFile(group.filename, 'rb') as fid:
        encoded_jpg = fid.read()
    encoded_jpg_io = io.BytesIO(encoded_jpg)
    image = Image.open(encoded_jpg_io)
    width, height = image.size

    filename = group.filename.encode('utf8')
    image_format = b'jpg'
    # check if the image format is matching with your images.
    xmins = []
    xmaxs = []
    ymins = []
    ymaxs = []
    classes_text = []
    classes = []

    for index, row in group.object.iterrows():
        xmins.append(row['xmin'] / width)
        xmaxs.append(row['xmax'] / width)
        ymins.append(row['ymin'] / height)
        ymaxs.append(row['ymax'] / height)
        classes_text.append(row['class'].encode('utf8'))
        classes.append(labels_to_int[row['class']])

    tf_example = tf.train.Example(features=tf.train.Features(feature={
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(filename),
        'image/source_id': dataset_util.bytes_feature(filename),
        'image/encoded': dataset_util.bytes_feature(encoded_jpg),
        'image/format': dataset_util.bytes_feature(image_format),
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
        'image/object/class/label': dataset_util.int64_list_feature(classes),
    }))
    return tf_example

def write_label_map(labels_to_int, path_to_label_map):
    with open(path_to_label_map, 'w') as f:
        for label in labels_to_int:
            id_ = labels_to_int[label]
            f.write('item { \n')
            f.write('\tname:\'{}\'\n'.format(label))
            f.write('\tid:{}\n'.format(id_))
            f.write('}\n')
#==================================================================================

#---------------------------Создание TF_Records------------------------------------




def main():
    path_to_label_map = os.path.join(train_data_folder, 'label_map.pbtxt')
    train_config_path = f'pretrained_models/{model_name}/train_pipeline.config'
    checkpoint_path = os.path.join(f'pretrained_models/{model_name}/checkpoints/', ckpt_file)
    config_path = f'pretrained_models/{model_name}/pipeline.config'

    if create_dataset:
        print('Создается TF Records файл')
        xml_df = xml_to_csv(path_to_imgs_xml, img_shape)
        xml_df.to_csv(f'{train_data_folder}/df_to_records.csv', index=False)

        labels = (pd.unique(xml_df['class']))
        labels_to_int = dict(zip(labels,
                                 list(range(1, len(labels) + 1))))

        writer = tf.io.TFRecordWriter(tf_records_path)
        # examples = pd.read_csv(csv_path, index_col=False)
        grouped = split(xml_df, 'filename')

        for group in grouped:
            tf_example = create_tf_example(group, labels_to_int)
            writer.write(tf_example.SerializeToString())

        writer.close()

        write_label_map(labels_to_int, path_to_label_map)

    if model_url:
        untar_name = model_url.split('/')[-1]
        untar_name = untar_name.split('.')[0]
        if os.path.exists(f'pretrained_models/{untar_name}') or os.path.exists(f'pretrained_models/{model_name}'):
            print('Такая модель уже существует')
        print('Загрузка модели')
        if not os.path.exists('pretrained_models'):
            os.mkdir('pretrained_models')
        wget.download(model_url, bar=bar_progress)
        tar_name = model_url.split('/')[-1]
        with tarfile.open(tar_name, 'r:gz') as tar:
            print('\nРаспаковка модели')
            def is_within_directory(directory, target):
                
                abs_directory = os.path.abspath(directory)
                abs_target = os.path.abspath(target)
            
                prefix = os.path.commonprefix([abs_directory, abs_target])
                
                return prefix == abs_directory
            
            def safe_extract(tar, path=".", members=None, *, numeric_owner=False):
            
                for member in tar.getmembers():
                    member_path = os.path.join(path, member.name)
                    if not is_within_directory(path, member_path):
                        raise Exception("Attempted Path Traversal in Tar File")
            
                tar.extractall(path, members, numeric_owner=numeric_owner) 
                
            
            safe_extract(tar, "pretrained_models")
            untar_name = tar_name.split('.')[0]
            os.rename(f'pretrained_models/{untar_name}', f'pretrained_models/{model_name}')
        print(f'Модель распакована в pretrained_models/{model_name}')
        os.rename(f'pretrained_models/{model_name}/checkpoint', f'pretrained_models/{model_name}/checkpoints')


    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
    with tf.io.gfile.GFile(config_path, "r") as f:
        proto_str = f.read()
        text_format.Merge(proto_str, pipeline_config)

    pipeline_config.model.ssd.num_classes = num_classes
    pipeline_config.train_config.batch_size = batch_size
    pipeline_config.train_config.fine_tune_checkpoint = checkpoint_path
    pipeline_config.train_config.fine_tune_checkpoint_type = "detection"
    pipeline_config.train_input_reader.label_map_path = path_to_label_map
    pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [tf_records_path]
    pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base = lr
    pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_learning_rate = lr_decay
    # pipeline_config.eval_input_reader[0].label_map_path = path_to_label_map
    # pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = ["my.records"]

    config_text = text_format.MessageToString(pipeline_config)
    with tf.io.gfile.GFile(train_config_path, "wb") as f:
        f.write(config_text)

    comand = f'python research/object_detection/model_main_tf2.py --model_dir=pretrained_models/{model_name} --pipeline_config_path={train_config_path} --num_train_steps={num_steps}'
    print(f'Для начала тренировки введите следующую команду:\n{comand}')



if __name__ == "__main__":
    main()
